# Pivot Studio笔试题

## 开放性问题

现阶段,机器学习被应用到计算机系统的方向有:

* 分布式机器学习(Distributed DNN Training)

  这方面的内容主要从两个角度切入,一个是Machine learning,一个是从System角度.

  从ML的角度做,主要是发明或者改进分布式训练算法,在保证分布式加速的同时仍能达到原来的学习效果(loss/accuracy).主要用到的方法包括优化(optimization)和统计学习理论(statistical learning theory).还有一类工作涉及到如何把单机算法改造成分布式,这要涉及到的问题是**如何降低分布式环境下的通信开销,提高加速比.**

  从System角度,主要是面对**计算量太大**的场景下(并行计算),可以多线程/多节点并行计算多节点共享公共的存储空间.对于**模型太大的场景**,需要把模型划分得到不同的节点上进行训练.

* 深度学习模型压缩/加速

  对DL model进行压缩主要考虑两个角度:减少计算量(例如conv层的计算量)/内存占用(NN的参数数量).

  从算法角度来加速,减少计算量即不仅要考虑ML的metric,还要考虑system层面的表现和功耗等.很多相关的工作是从ML的角度压缩模型的(Arch Compression);

  从System角度来加速的方向有:通过Quantized(量化)降低计算精度要求;引入新硬件,DL Acclerator;矩阵算子优化等

* 存储方面

  ML训练数据的规模一般很庞大,如何为ML设计一个专用的文件系统或者数据库,以便于加速ML读取数据的速度

  此外还有在ML framework中,以及parameter server中,需要一个KV storage system来存储parameters,针对ML场景下来优化这个KV存储系统.

* 用ML优化传统的system问题,大部分是用ML去优化一个传统的system问题中,一些需要人工经验调整,或者可以从历史情况中学到一些知识的模块,比如数据库参数,操作系统页表,数据库索引等等,即**一个模块可以被ML化的前提是它必须是empirical的**.

  ML优化system方向还有寻找system界更多可以ML化的场景,有一类思路是将ML深度集成到系统设计中,例如Self-Driving Database概念

我认为比较合理的方向是用ML优化一些传统的system问题,例如在自动驾驶汽车数据库管理系统上应用ML.以前的预测技术对查询的资源利用率进行建模。然而，每当数据库的物理设计和硬件资源发生变化时，这些指标就会发生变化，从而使以前的预测模型变得无用.可以使用LSTM可以预测未来的workload pattern,从而解决这一问题.LSTM长短期记忆,是一种特殊的循环网络(RNN),主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题,比普通的RNN在更长序列中有更好的表现.算法具体见[LSTM讲解](https://zhuanlan.zhihu.com/p/32085405) (俺也不懂).虽然该算法可以很好地解决这个问题,但是需要GPU来进行训练网络,这就体现了它的局限性,即生产环境下要求数据库服务器安装显卡是不现实的.工程上的一个解决方案是搞一个集中式的训练集群,在数据集即服务(DBaaS)的情况下行得通,但是对外发布的数据库产品中就不行了.这是其不可忽视的局限性.



## 场景题

1. 数据集来源可以来自网上各大平台上用户使用的表情包,图片或者许多网站上的资源等等,因为大平台上能传播的图片一般都是能过审的,依次可以打上正确的label,此外还可以去爬点不能过审的图片?标记上错误的label,这样也可减少给予图片标签需要的人工投入;一般来说,表情包和正常图片内容差别很大,应当分开单独处理,这里只讨论对正常图片集的设置,将服从于同一个分布的图片集分成2部分::training_set,

   test_set;将图片集都转换成三信道RGB的规格大小相同的图片.

2. 采用CNN卷积神经网络,因为任务以识别图片内容为主,用深度学习参数规模实在太大了,不如CNN识别高效.算法原理就是利用kernel_size为(n * n),out_channel为m的矩阵作为卷积核,training_set中图片经过卷积后输出channel为m的矩阵,再通过activation function(ReLU)对数据进行处理,再投入池化层中,池化层一般使用Maxpooling;如此反复几轮后输出的矩阵经过flatten展成一串数据再进入fully_connected的神经网络中进行forward propagation和backward propagation经过多轮gradient descent后,训练得出最后的model,将test_set的图片应用到model中,计算出accuracy,看结果如何.

3. 利用BERT联合视觉和语言之间的信息,以视觉和语言嵌入特征作为输入。在输入中，每个元素要么来自输入句子的单词，要么来自输入图像的某个区域(RoI)，以及某些特殊元素[CLS]、[SEP]、[END]等用来消除不同的输入格式的歧义。

   * token embedding：遵循BERT的设置，不同之处在于，对于视觉元素，为每一个视觉元素分配一个特殊的[IMG]token。
   * visual features embedding：每一个输入元素都有一个对应的视觉特征嵌入，视觉特征嵌入是视觉外观特征和视觉几何特征的concatenation。通过应用Fast R-CNN检测器来提取视觉外观特征。每个RoI输出层之前的特征向量被用作视觉特征嵌入。对于非视觉元素，相应的视觉外观特征是在整个输入图像上提取的特征，它们是通过在覆盖整个输入图像的RoI上应用Faster R-CNN获得的。视觉几何特征旨在定义每个视觉元素的几何位置。每个RoI的几何特征是一个四维向量，分别用左上角和右下角的坐标分别除以图像的宽度和高度。
   * segment embedding：定义了三种类型的句段A，B，C，以将来自不同来源的输入元素分开。A表示来自于第一个输入句子中的单词，B表示来自于第二个输入句子中的单词，C表示来自于输入图像的RoI。
   * position embedding：与BERT一样，将序列位置嵌入添加到每个输入元素中，以指示其在输入序列中的顺序。由于输入的视觉元素之间没有自然顺序，因此它们在输入序列中的任何排列都应获得相同的结果。因此，所有视觉元素的序列位置嵌入都相同。

   ![image-20221002230819033](ReadMe.assets/image-20221002230819033.png)

4. 样本数据太小会导致underfit(欠拟合),导致模型表现较差

   解决办法:

   有基于数据增强的方法,基于模型改进的方法,基于算法优化的方法

   首先可以通过对平移裁剪翻转加噪音等方法增强数据;或者根据我们的小样本数据集是图片集,在网上找到相似的数据集,可以训练一个GAN网络,通过学习给小样本数据集加上扰动来生成新样本;或者基于原本的小样本,训练一个transformer学习样本之间的变化,然后使用该transformer对小样本数据集进行扩充;

   **多任务学习**:违规的信息肯定有很多种,利用多任务模型并行处理识别任务

   **基于外部记忆的学习**：通过对小样本数据集学习得到知识，然后存储到外部，对于新样本，都使用存储在外部的知识进行表示，并根据表示来完成匹配。这种方法大大降低假设空间；

   **改善已有参数。**这种方法从参数初始化的角度着手，主要思路是借助已训练好的模型参数来调整小样本模型的参数，例如：在大数据集训练好模型来初始化小样本模型；聚合其他已训练好的模型到一个模型；给已训练好的模型加一些特用于小样本任务的参数；等等



## 代码题

[MNIST手写数据集识别](https://github.com/lowa9/MNIST-Classification)